{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis with IMDB reviews dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rBryac_DzDl"
      },
      "source": [
        "#**Task 0**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ6ITydTwB4X"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import numpy as np\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRT8euvNwZeL",
        "outputId": "a51a759f-0a74-4b4c-8de4-ba70b2f25c65"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hNRK9YywgZI"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNbN6sacwihL",
        "outputId": "51c232f4-aed3-4cfb-869d-60c9b887cb22"
      },
      "source": [
        "os.listdir(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'imdb.vocab', 'README', 'imdbEr.txt', 'train']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWPWslJuwsv0",
        "outputId": "44738b1b-0c5f-4356-94a7-ed9fa833a340"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_unsup.txt',\n",
              " 'urls_neg.txt',\n",
              " 'neg',\n",
              " 'unsup',\n",
              " 'urls_pos.txt',\n",
              " 'pos',\n",
              " 'labeledBow.feat',\n",
              " 'unsupBow.feat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkqT8lWKwvnq",
        "outputId": "f9f813c8-3da0-4522-8334-e8d5e0b14812"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfjWjH-pwyvX"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKq0jRpDw3fe",
        "outputId": "96c752e0-17f1-4461-b403-62c466bfbf9d"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', \n",
        "    batch_size=batch_size, \n",
        "    seed=seed)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZqGzBn0ahcb",
        "outputId": "4079ba2d-f593-4762-8073-8fa3712ee769"
      },
      "source": [
        "#625*32=20,000 reviews\n",
        "len(raw_train_ds)\n",
        "#No.85"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K64w4k2xJFM",
        "outputId": "eabb91b0-c724-41c2-f986-92f33e4f3a85"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review b\"Mild Spoilers<br /><br />In the near future, Arnold stars as Ben Richards, a wrongly convicted man coerced into playing 'The Running Man', a deadly TV game show where people have to keep moving to try and escape brutal deaths at the hands of the 'Stalkers'. Of course, people are expected to die eventually and its up to Arnold to prove the system wrong.<br /><br />I haven't read the Stephen King book, but this is a great film regardless, one of Arnold's best. He does what he does best in the action man role, delivering death with unforgettable one-liners. Classics are probably the 'He was a real pain in the neck' after strangling a guy with barb wire, and 'He had to split!', referring to whereabouts he just chain sawed someone vertically. Dawson is perfectly irritating as the TV presenter, and all the 'Stalkers' are suitably camp. The action is violent, but its an action film. That's the point. The film is fast paced, and at 90 minutes it doesn't overstay its welcome. <br /><br />With Starsky and Hutch's Paul Michael Glaser at the helm, and made in the wake of the success of The Terminator, previously this film was probably seen as just another mindless action vehicle for Arnold, and very far fetched. But today, anyone who watches a lot of TV could see how the film is getting closer to reality. I wouldn't be surprised if I turn on the TV in the 'near future' and see a show not to far from this.<br /><br />On that depressing note, I must however recommend 'The Running Man' to anyone who likes the 80s, Arnold, ridiculous acts or violence or just a good action film. 9. 5 / 10\"\n",
            "Label 1\n",
            "Review b\"To get in touch with the beauty of this film pay close attention to the sound track, not only the music, but the way all sounds help to weave the imagery. How beautifully the opening scene leading to the expulsion of Gino establishes the theme of moral ambiguity! Note the way music introduces the characters as we are led inside Giovanna's marriage. Don't expect to find much here of the political life of Italy in 1943. That's not what this is about. On the other hand, if you are susceptible to the music of images and sounds, you will be led into a word that reaches beyond neo-realism. By the end of the film we there are moments Antonioni-like landscape that has more to do with the inner life of the characters than with real places. This is one of my favorite Visconti films.\"\n",
            "Label 1\n",
            "Review b'It is rare that one comes across a movie as flawless as this. It\\'s truly one of the best acted, most tightly structured films I\\'ve ever seen. Every line of dialogue can be interpreted in several ways, relating to each of the three main characters differently. The film weaves an intrinsic web of motivations and double crosses that snare you and refuse to let go. Add to this that the slow-burning romance between Kevin and Faye is as moving as anything that\\'s ever been committed to celluloid and you have the ingredients for a perfect film. It exposes the romance of movies such as \"Titanic\" as the trite cliches they are. If you\\'re looking for a movie to watch while you fold laundry, this isn\\'t it. You have to commit yourself to this film. You can\\'t have a conversation while running in and out of the room. This movie demands your attention. Treat it with the respect you deserve and you\\'ll get a lot out of it. Unless you think \"Titanic\" is the greatest film ever.'\n",
            "Label 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BzzXmeXxRc-",
        "outputId": "fc8eaac4-7a5c-4d77-ad23-edfe7585e480"
      },
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to neg\n",
            "Label 1 corresponds to pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScqlAiC6xWEo",
        "outputId": "033adf9a-7d93-43fb-fb22-90e8b3d0669a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9iRtNGzxcq4",
        "outputId": "4013ddd1-4ca2-48ba-e311-8b48397d5f26"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test', \n",
        "    batch_size=batch_size)\n",
        "#782 batches, 781*32+ 24=25,000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zBPlnNZxiHR"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),\n",
        "                                  '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR2YE_OOxnKY"
      },
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "embedding_dim=128\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-8MBokjxpPQ"
      },
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plB5VEaJxsP7"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW2BSLNBxvax",
        "outputId": "dbd16bbd-c39c-4604-a046-89316e450f51"
      },
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review tf.Tensor(b\"The 60\\xc2\\xb4s is a well balanced mini series between historical facts and a good plot. In four deliveries, we follow a north American family, with 3 members. But we don't only see them. We also follow the story of several characters as a black reverend, an extremist student leader, and a soldier in Vietnam. The filmography is just extraordinary. In the first chapters, we see some shots of the Vietnam war, in between the scenes. The next chapter, doesn't start where the last one finished, it starts some time after, giving us a little mystery on what happened. In general, The 60\\xc2\\xb4s mini series, is a must see, not only for hippies fanatics, but for everyone with little curiosity about the topic.\", shape=(), dtype=string)\n",
            "Label pos\n",
            "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
            "array([[   2,    1,    7,    4,   73, 7844, 4199,  200,  188, 1397, 2232,\n",
            "           3,    4,   49,  111,    8,  695,    1,   71,  821,    4, 2324,\n",
            "         312,  215,   16,  452, 1038,   18,   71,   89,   61,   67,   93,\n",
            "          71,   77,  821,    2,   63,    5,  440,  100,   14,    4,  321,\n",
            "           1,   33,    1, 1528, 2032,    3,    4, 1560,    8, 2503,    2,\n",
            "        7382,    7,   40, 2735,    8,    2,   83, 9141,   71,   67,   46,\n",
            "         647,    5,    2, 2503,  329,    8,  188,    2,  136,    2,  357,\n",
            "        4745,  144,  368,  114,    2,  229,   28, 1752,    9,  501,   46,\n",
            "          58,  101,  725,  167,    4,  112,  743,   20,   48,  553,    8,\n",
            "         776,    2,    1, 4199,  200,    7,    4,  219,   67,   21,   61,\n",
            "          15, 6115, 9349,   18,   15,  302,   16,  112, 3496,   42,    2,\n",
            "        2936,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5D1JUy0xx6y",
        "outputId": "0e0d3128-4af5-4d84-978c-150cf8feae4b"
      },
      "source": [
        "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
        "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1287 --->  silent\n",
            " 313 --->  night\n",
            "Vocabulary size: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naV3rsi8x5wb"
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text) #80% training dataset :20,000\n",
        "val_ds = raw_val_ds.map(vectorize_text) #20% training dataset :5,000\n",
        "test_ds = raw_test_ds.map(vectorize_text) #100% testing dataset: 25,000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IohWAFfhx-Xu"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rShphuLaClau",
        "outputId": "24b601e8-e292-4fae-95d9-f44f3ec3bdff"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, 16),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160016    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 16)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLVU6NmJCltx"
      },
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1HmRXI4Cl5X",
        "outputId": "c30e2a80-ab22-4a8b-c7d8-d71424bb41a8"
      },
      "source": [
        "epochs = 10\n",
        "history2 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 12s 18ms/step - loss: 0.6820 - binary_accuracy: 0.6196 - val_loss: 0.6126 - val_binary_accuracy: 0.7754\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.5766 - binary_accuracy: 0.7893 - val_loss: 0.4958 - val_binary_accuracy: 0.8238\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.4639 - binary_accuracy: 0.8375 - val_loss: 0.4185 - val_binary_accuracy: 0.8466\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3917 - binary_accuracy: 0.8624 - val_loss: 0.3729 - val_binary_accuracy: 0.8606\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3426 - binary_accuracy: 0.8773 - val_loss: 0.3444 - val_binary_accuracy: 0.8670\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.3093 - binary_accuracy: 0.8889 - val_loss: 0.3255 - val_binary_accuracy: 0.8712\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2857 - binary_accuracy: 0.8969 - val_loss: 0.3123 - val_binary_accuracy: 0.8726\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2652 - binary_accuracy: 0.9031 - val_loss: 0.3031 - val_binary_accuracy: 0.8766\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2476 - binary_accuracy: 0.9116 - val_loss: 0.2965 - val_binary_accuracy: 0.8770\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2335 - binary_accuracy: 0.9160 - val_loss: 0.2917 - val_binary_accuracy: 0.8796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhxBRvbKCmE3",
        "outputId": "e4e7ed6c-19b4-47c7-f39e-41be499a9496"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 10s 12ms/step - loss: 0.3106 - binary_accuracy: 0.8731\n",
            "Loss:  0.31058719754219055\n",
            "Accuracy:  0.8731200098991394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9i3GIRseb30",
        "outputId": "92816443-fcae-4d7d-a07f-a8c924cf4026"
      },
      "source": [
        "history_dict = history2.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkD_lcIXDpAq"
      },
      "source": [
        "# **Task1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaXzWQXBzccB"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, kernel_size=(5//2)+5, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "CNN1 = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "CNN1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elioIMsOyDHg",
        "outputId": "8b7865dc-0805-4b91-fa6b-c8f11318cde6"
      },
      "source": [
        "CNN1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 128)         1280000   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 128)         114816    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,411,457\n",
            "Trainable params: 1,411,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNvhYlGG1gn1",
        "outputId": "44ed8407-f160-4f65-885d-c0b8d921b5f0"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "CNN1.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 53s 84ms/step - loss: 0.6275 - accuracy: 0.5944 - val_loss: 0.3320 - val_accuracy: 0.8596\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 52s 84ms/step - loss: 0.3188 - accuracy: 0.8670 - val_loss: 0.3034 - val_accuracy: 0.8754\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.1938 - accuracy: 0.9268 - val_loss: 0.3408 - val_accuracy: 0.8690\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.1239 - accuracy: 0.9563 - val_loss: 0.4017 - val_accuracy: 0.8688\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.0842 - accuracy: 0.9678 - val_loss: 0.4659 - val_accuracy: 0.8690\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.0443 - accuracy: 0.9847 - val_loss: 0.4870 - val_accuracy: 0.8656\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.0386 - accuracy: 0.9860 - val_loss: 0.5748 - val_accuracy: 0.8698\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.0371 - accuracy: 0.9873 - val_loss: 0.6532 - val_accuracy: 0.8640\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.0314 - accuracy: 0.9887 - val_loss: 0.6145 - val_accuracy: 0.8626\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 52s 83ms/step - loss: 0.0287 - accuracy: 0.9900 - val_loss: 0.6095 - val_accuracy: 0.8604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffa9451f8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MCv5fml2EJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0076ef05-a8cf-401a-f8ba-cc260b523917"
      },
      "source": [
        "CNN1.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 11s 14ms/step - loss: 0.6941 - accuracy: 0.8450\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6941274404525757, 0.8449599742889404]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAc0whdkEERA"
      },
      "source": [
        "# **Task 2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk3jpSGL2G-e"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, kernel_size=(5//2)+5-1, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.Conv1D(128, kernel_size=(5//2)+5-1, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "CNN2 = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "CNN2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxlqz77YEeF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d6f55a-4896-40ab-813a-90fab0b3584f"
      },
      "source": [
        "CNN2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, None, 128)         1280000   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 128)         98432     \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 128)         98432     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,493,505\n",
            "Trainable params: 1,493,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AAiI6HcEeaE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "ce714eee-5f95-4313-943e-92af943fe4fb"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "CNN2.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 55s 87ms/step - loss: 0.6311 - accuracy: 0.5903 - val_loss: 0.3367 - val_accuracy: 0.8532\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 54s 87ms/step - loss: 0.3187 - accuracy: 0.8706 - val_loss: 0.3174 - val_accuracy: 0.8656\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 55s 87ms/step - loss: 0.2013 - accuracy: 0.9244 - val_loss: 0.3941 - val_accuracy: 0.8642\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 54s 87ms/step - loss: 0.1175 - accuracy: 0.9561 - val_loss: 0.4775 - val_accuracy: 0.8628\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 55s 87ms/step - loss: 0.0800 - accuracy: 0.9702 - val_loss: 0.5176 - val_accuracy: 0.8530\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 55s 87ms/step - loss: 0.0464 - accuracy: 0.9828 - val_loss: 0.7129 - val_accuracy: 0.8586\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 54s 87ms/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 0.7015 - val_accuracy: 0.8526\n",
            "Epoch 8/10\n",
            "223/625 [=========>....................] - ETA: 33s - loss: 0.0315 - accuracy: 0.9880"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-2ae00bab7106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fit the model using the train and test datasets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mCNN2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8E1pOwDEei9"
      },
      "source": [
        "CNN2.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9U2Fc2qIkwn"
      },
      "source": [
        "# **Task3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyn4DfSSEemO"
      },
      "source": [
        "# 20% of training dataset 125*32=4,000\n",
        "train_ds_20=train_ds.take(125)\n",
        "\n",
        "# 40% of training dataset 250*32=8,000\n",
        "train_ds_40=train_ds.take(250)\n",
        "\n",
        "# 60% of training dataset 375*32=12,000\n",
        "train_ds_60=train_ds.take(375)\n",
        "\n",
        "# 80% of training dataset 500*32=16,000\n",
        "train_ds_80=train_ds.take(500)\n",
        "\n",
        "# 100% of training dataset 625*32=20,000\n",
        "train_ds_100=train_ds.take(625)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMOke4ZjM4B1"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# A integer input for vocab indices.\n",
        "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "# 'embedding_dim'.\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Conv1D + global max pooling\n",
        "x = layers.Conv1D(128, kernel_size=(5//2)+5, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "\n",
        "CNNbest = tf.keras.Model(inputs, predictions)\n",
        "\n",
        "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "CNNbest.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyYJonwSQgI-"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "# Fit the model using the train and test datasets.\n",
        "history_20=CNNbest.fit(train_ds_20, validation_data=val_ds, epochs=epochs)\n",
        "CNNbest.evaluate(test_ds)\n",
        "history_20_dict = history_20.history\n",
        "history_20_dict.keys()\n",
        "\n",
        "#------------------------------------------------------------------#\n",
        "\n",
        "history_40=CNNbest.fit(train_ds_40, validation_data=val_ds, epochs=epochs)\n",
        "CNNbest.evaluate(test_ds)\n",
        "history_40_dict = history_40.history\n",
        "history_40_dict.keys()\n",
        "\n",
        "#------------------------------------------------------------------#\n",
        "\n",
        "history_60=CNNbest.fit(train_ds_60, validation_data=val_ds, epochs=epochs)\n",
        "CNNbest.evaluate(test_ds)\n",
        "history_60_dict = history_60.history\n",
        "history_60_dict.keys()\n",
        "\n",
        "#------------------------------------------------------------------#\n",
        "\n",
        "history_80=CNNbest.fit(train_ds_80, validation_data=val_ds, epochs=epochs)\n",
        "CNNbest.evaluate(test_ds)\n",
        "history_80_dict = history_80.history\n",
        "history_80_dict.keys()\n",
        "#------------------------------------------------------------------#\n",
        "\n",
        "history_100=CNNbest.fit(train_ds_100, validation_data=val_ds, epochs=epochs)\n",
        "CNNbest.evaluate(test_ds)\n",
        "history_100_dict = history_100.history\n",
        "history_100_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAPjmqjdVmSD"
      },
      "source": [
        "acc_20 = history_20_dict['accuracy']\n",
        "val_acc_20 = history_20_dict['val_accuracy']\n",
        "\n",
        "acc_40 = history_40_dict['accuracy']\n",
        "val_acc_40 = history_40_dict['val_accuracy']\n",
        "\n",
        "acc_60 = history_60_dict['accuracy']\n",
        "val_acc_60 = history_60_dict['val_accuracy']\n",
        "\n",
        "acc_80 = history_80_dict['accuracy']\n",
        "val_acc_80 = history_80_dict['val_accuracy']\n",
        "\n",
        "acc_100 = history_100_dict['accuracy']\n",
        "val_acc_100 = history_100_dict['val_accuracy']\n",
        "\n",
        "epochs = range(1, 11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezMl6zDCk1J9"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "plt.plot(epochs, acc_20, 'bo', label='Training acc_20')\n",
        "plt.plot(epochs, acc_40, 'go', label='Training acc_40')\n",
        "plt.plot(epochs, acc_60, 'ro', label='Training acc_60')\n",
        "plt.plot(epochs, acc_80, 'co', label='Training acc_80')\n",
        "plt.plot(epochs, acc_100, 'mo', label='Training acc_100')\n",
        "\n",
        "#b,g,r,c,m\n",
        "plt.plot(epochs, val_acc_20, 'b', label='Validation acc')\n",
        "plt.plot(epochs, val_acc_40, 'g', label='Validation acc')\n",
        "plt.plot(epochs, val_acc_60, 'r', label='Validation acc')\n",
        "plt.plot(epochs, val_acc_80, 'c', label='Validation acc')\n",
        "plt.plot(epochs, val_acc_100, 'm', label='Validation acc')\n",
        "\n",
        "\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYFZNfMZqOdk"
      },
      "source": [
        "# **Task 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsV-D8jsmKXJ"
      },
      "source": [
        "y_pred_prob = CNNbest.predict(test_ds)\n",
        "y_pred_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR1tY7xLsN5j"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(y_pred_prob, columns = ['Probability'])\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on3ClRmFszmn"
      },
      "source": [
        "df=df.sort_values(by='Probability', ascending=False)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcMftX8dtP2B"
      },
      "source": [
        "df_pos=df.head(20)\n",
        "df_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FRVFKvFtlBA"
      },
      "source": [
        "df_neg=df.tail(20)\n",
        "df_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA1FxAEntpxF"
      },
      "source": [
        "index_pos = df_pos.index\n",
        "index_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmphBRBou49P"
      },
      "source": [
        "index_neg = df_neg.index\n",
        "index_neg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7JlGZEY134U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}